{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSHEnsemble\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_FILE = \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/IMDB_Base.csv\"\n",
    "NEW_FILE = \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/Versions/imdb_with_awards.csv\"\n",
    "CANDIDATE_DIR = \"sinteticDB/IMDB/externalTables\"\n",
    "MAIN_DATASET_KEYS = [\"Series_Title\"]\n",
    "NUM_PERM = 128\n",
    "\n",
    "def create_minhash(values: List[str], num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for v in values:\n",
    "        if pd.notna(v):\n",
    "            m.update(str(v).strip().lower().encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def extract_column_dataframes(directory: str) -> List[Tuple[str, str, List[str], int]]:\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            table_name = filename[:-4]\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            for col in df.columns:\n",
    "                values = df[col].astype(str).dropna().unique().tolist()\n",
    "                results.append((table_name, col, values, len(values)))\n",
    "    return results\n",
    "\n",
    "def recommend_join_type(candidate_keys, main_keys, jaccard_sim, coverage_threshold=0.7):\n",
    "    key_overlap = len(set(candidate_keys).intersection(set(main_keys)))\n",
    "    if key_overlap == len(main_keys):\n",
    "        return \"inner join\"\n",
    "    elif key_overlap > 0 and jaccard_sim > coverage_threshold:\n",
    "        return \"left join\"\n",
    "    else:\n",
    "        return \"outer join\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load base and new dataset versions ---\n",
    "base_df = pd.read_csv(BASE_FILE)\n",
    "new_df = pd.read_csv(NEW_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï New attributes detected: Award_Name, Won, Award_Year, Award_Category\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Detect newly added attributes ---\n",
    "base_cols = set(base_df.columns)\n",
    "new_cols = set(new_df.columns)\n",
    "added_cols = new_cols - base_cols\n",
    "\n",
    "if not added_cols:\n",
    "    print(\"‚úÖ No new attributes found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"üÜï New attributes detected: {', '.join(added_cols)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MinHashLSHEnsemble.index() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      9\u001b[39m     index_metadata.append({\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m'\u001b[39m: table_name,\n\u001b[32m     11\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m'\u001b[39m: col_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mkeys\u001b[39m\u001b[33m'\u001b[39m: [col_name]  \u001b[38;5;66;03m# simple assumption\u001b[39;00m\n\u001b[32m     15\u001b[39m     })\n\u001b[32m     17\u001b[39m lsh = MinHashLSHEnsemble(threshold=\u001b[32m0.1\u001b[39m, num_perm=NUM_PERM,num_part=\u001b[32m32\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mlsh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfull_name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex_metadata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminhashes\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: MinHashLSHEnsemble.index() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# --- Index external candidate columns with MinHash + LSH Ensemble ---\n",
    "column_entries = extract_column_dataframes(CANDIDATE_DIR)\n",
    "minhashes = []\n",
    "index_metadata = []\n",
    "\n",
    "for table_name, col_name, values, size in column_entries:\n",
    "    mh = create_minhash(values, NUM_PERM)\n",
    "    minhashes.append(mh)\n",
    "    index_metadata.append({\n",
    "        'table': table_name,\n",
    "        'column': col_name,\n",
    "        'full_name': f\"{table_name}.{col_name}\",\n",
    "        'size': size,\n",
    "        'keys': [col_name]  # simple assumption\n",
    "    })\n",
    "\n",
    "lsh = MinHashLSHEnsemble(threshold=0.1, num_perm=NUM_PERM,num_part=32)\n",
    "lsh.index(\n",
    "    [(m['full_name'], m['size']) for m in index_metadata],\n",
    "    minhashes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possibile fix\n",
    "# --- Index external candidate columns with MinHash + LSH Ensemble ---\n",
    "column_entries = extract_column_dataframes(CANDIDATE_DIR)\n",
    "minhashes = []\n",
    "index_metadata = []\n",
    "\n",
    "for table_name, col_name, values, size in column_entries:\n",
    "    mh = create_minhash(values, NUM_PERM)\n",
    "    minhashes.append(mh)\n",
    "    index_metadata.append({\n",
    "        'table': table_name,\n",
    "        'column': col_name,\n",
    "        'full_name': f\"{table_name}.{col_name}\",\n",
    "        'size': size,\n",
    "        'keys': [col_name]  # simple assumption\n",
    "    })\n",
    "\n",
    "lsh = MinHashLSHEnsemble(threshold=0.5, num_perm=NUM_PERM)\n",
    "keys = [m['full_name'] for m in index_metadata]\n",
    "sizes = [m['size'] for m in index_metadata]\n",
    "combined = list(zip(keys, minhashes, sizes))\n",
    "lsh.index(combined) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing new attribute: `Award_Name`\n",
      "awards.Award_Name\n",
      "country_data.Localized_Title\n",
      "‚ùå No good matches found for this attribute.\n",
      "\n",
      "üîç Analyzing new attribute: `Won`\n",
      "streaming.Subscription_Required\n",
      "awards.Won\n",
      "country_data.Localized_Title\n",
      "‚ùå No good matches found for this attribute.\n",
      "\n",
      "üîç Analyzing new attribute: `Award_Year`\n",
      "‚ùå No good matches found for this attribute.\n",
      "\n",
      "üîç Analyzing new attribute: `Award_Category`\n",
      "awards.Award_Category\n",
      "country_data.Localized_Title\n",
      "‚ùå No good matches found for this attribute.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--- For each new attribute: search + recommend join ---\n",
    "for new_col in added_cols:\n",
    "    print(f\"üîç Analyzing new attribute: `{new_col}`\")\n",
    "\n",
    "    new_values = new_df[new_col].astype(str).dropna().unique().tolist()\n",
    "    if not new_values:\n",
    "        print(\"‚ö†Ô∏è No values found for this attribute. Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    new_attr_minhash = create_minhash(new_values, NUM_PERM)\n",
    "    candidates = lsh.query(new_attr_minhash, len(new_values))\n",
    "    for key in lsh.query(new_attr_minhash, len(new_values)):\n",
    "        print(key)\n",
    "\n",
    "    ranked = []\n",
    "    for meta, mh in zip(index_metadata, minhashes):\n",
    "        if meta['full_name'] in candidates:\n",
    "            sim = new_attr_minhash.jaccard(mh)\n",
    "            join_type = recommend_join_type(meta['keys'], MAIN_DATASET_KEYS, sim)\n",
    "            ranked.append((meta['table'], meta['column'], sim, join_type))\n",
    "\n",
    "    ranked.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    if ranked:\n",
    "        print(\"\\nTop Candidate Matches:\")\n",
    "        for table, column, sim, join_type in ranked[:5]:\n",
    "            print(f\" ‚Üí {table}.{column} | Jaccard: {sim:.4f} | Join: {join_type}\")\n",
    "\n",
    "        best = ranked[0]\n",
    "        print(f\"\\n‚úÖ Best match for `{new_col}`: {best[0]}.{best[1]} ‚Üí {best[3]} (Sim: {best[2]:.4f})\\n\")\n",
    "    else:\n",
    "        print(\"‚ùå No good matches found for this attribute.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
