{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSHEnsemble\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_FILE = \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/IMDB_Base.csv\"\n",
    "NEW_FILE = \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/Versions/imdb_with_financials_and_directors.csv\"\n",
    "CANDIDATE_DIR = \"sinteticDB/IMDB/externalTables\"\n",
    "MAIN_DATASET_KEYS = [\"Series_Title\"]\n",
    "NUM_PERM = 128\n",
    "\n",
    "def create_minhash(values: List[str], num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for v in values:\n",
    "        if pd.notna(v):\n",
    "            m.update(str(v).strip().lower().encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def extract_column_dataframes(directory: str) -> List[Tuple[str, str, List[str], int]]:\n",
    "    results = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            table_name = filename[:-4]\n",
    "            df = pd.read_csv(os.path.join(directory, filename))\n",
    "            for col in df.columns:\n",
    "                values = df[col].astype(str).dropna().unique().tolist()\n",
    "                #results.append((table_name, col, values, len(values)))\n",
    "                results.append((table_name, col, values, len(values),len(df)))\n",
    "    return results\n",
    "\n",
    "def get_table(name):\n",
    "    name=name+\".csv\"\n",
    "    table = pd.read_csv(os.path.join(CANDIDATE_DIR, name))\n",
    "    return table\n",
    "\n",
    "def recommend_join_type(candidate_keys, main_keys, jaccard_sim, coverage_threshold=0.7):\n",
    "    key_overlap = len(set(candidate_keys).intersection(set(main_keys)))\n",
    "    if key_overlap == len(main_keys):\n",
    "        return \"inner join\"\n",
    "    elif key_overlap > 0 and jaccard_sim > coverage_threshold:\n",
    "        return \"left join\"\n",
    "    else:\n",
    "        return \"outer join\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION 2 reccomend join type\n",
    "def estimate_join_type_with_confidence(orig_rows, ext_rows, joined_rows):\n",
    "    results = []\n",
    "    \n",
    "    # Inner join likelihood\n",
    "    if joined_rows <= min(orig_rows, ext_rows):\n",
    "        confidence = min(90, 100 * (1 - (joined_rows/min(orig_rows, ext_rows))))\n",
    "        results.append((\"inner\", confidence))\n",
    "    \n",
    "    # Left join likelihood\n",
    "    if joined_rows == orig_rows:\n",
    "        results.append((\"left\", 80))\n",
    "    elif abs(joined_rows - orig_rows) < orig_rows * 0.1:  # within 10%\n",
    "        results.append((\"left\", 60))\n",
    "    \n",
    "    # Right join likelihood\n",
    "    if joined_rows == ext_rows:\n",
    "        results.append((\"right\", 80))\n",
    "    elif abs(joined_rows - ext_rows) < ext_rows * 0.1:  # within 10%\n",
    "        results.append((\"right\", 60))\n",
    "    \n",
    "    # Full outer likelihood\n",
    "    if joined_rows >= max(orig_rows, ext_rows):\n",
    "        expected_min = max(orig_rows, ext_rows)\n",
    "        expected_max = orig_rows + ext_rows\n",
    "        if expected_min == expected_max:\n",
    "            confidence = 0  # tables disjoint\n",
    "        else:\n",
    "            confidence = 100 * (1 - abs(joined_rows - (expected_min+expected_max)/2)/(expected_max-expected_min))\n",
    "        results.append((\"full\", min(90, confidence)))\n",
    "    \n",
    "    if not results:\n",
    "        return [(\"unknown\", 0)]\n",
    "    \n",
    "    # Sort by confidence\n",
    "    return sorted(results, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_joinable_attributes(df1: pd.DataFrame, df2: pd.DataFrame, \n",
    "                            sample_size: int = 1000, min_overlap: float = 0.1) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Find potential joinable attributes between two DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df1: First DataFrame\n",
    "        df2: Second DataFrame\n",
    "        sample_size: Number of rows to sample for value matching (0 for full comparison)\n",
    "        min_overlap: Minimum ratio of overlapping values to consider columns joinable\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples with matching column pairs (df1_column, df2_column)\n",
    "    \"\"\"\n",
    "    potential_matches = []\n",
    "    \n",
    "    # Step 1: Find columns with matching names and compatible dtypes\n",
    "    common_cols = set(df1.columns) & set(df2.columns)\n",
    "    for col in common_cols:\n",
    "        if pd.api.types.is_dtype_equal(df1[col].dtype, df2[col].dtype):\n",
    "            potential_matches.append((col, col,0))\n",
    "    \n",
    "    # Step 2: Find columns with compatible dtypes (even if names don't match)\n",
    "    for col1 in df1.columns:\n",
    "        for col2 in df2.columns:\n",
    "            # Skip if already found or same column pair\n",
    "            if (col1, col2) in potential_matches or col1 == col2:\n",
    "                continue\n",
    "            \n",
    "            # Check dtype compatibility\n",
    "            if pd.api.types.is_dtype_equal(df1[col1].dtype, df2[col2].dtype):\n",
    "                # Get samples (handle empty DataFrames)\n",
    "                sample1 = _get_sample(df1[col1].dropna(), sample_size)\n",
    "                sample2 = _get_sample(df2[col2].dropna(), sample_size)\n",
    "                \n",
    "                # Skip if either sample is empty\n",
    "                if len(sample1) == 0 or len(sample2) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Check for overlapping values\n",
    "                common_values = set(sample1) & set(sample2)\n",
    "                if len(common_values) > 0:\n",
    "                    overlap_ratio = len(common_values) / min(len(sample1.unique()), len(sample2.unique()))\n",
    "                    if overlap_ratio >= min_overlap:\n",
    "                        potential_matches.append((col1, col2,overlap_ratio))\n",
    "        potential_matches.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return potential_matches\n",
    "\n",
    "def _get_sample(series: pd.Series, sample_size: int) -> pd.Series:\n",
    "    \"\"\"Helper function to get sample from a series.\"\"\"\n",
    "    if sample_size <= 0 or len(series) <= sample_size:\n",
    "        return series\n",
    "    return series.sample(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load base and new dataset versions ---\n",
    "base_df = pd.read_csv(BASE_FILE)\n",
    "new_df = pd.read_csv(NEW_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🆕 New attributes detected: Director_Awards, Director_Birth_Year, Director_Nationality, Director_Gender, Director_Name, Profitability_Ratio, Release_Season, Production_Budget, Primary_Production_Company, Box_Office_Gross\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Detect newly added attributes ---\n",
    "base_cols = set(base_df.columns)\n",
    "new_cols = set(new_df.columns)\n",
    "added_cols = new_cols - base_cols\n",
    "\n",
    "if not added_cols:\n",
    "    print(\"✅ No new attributes found.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"🆕 New attributes detected: {', '.join(added_cols)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Index external candidate columns with MinHash + LSH Ensemble ---\n",
    "column_entries = extract_column_dataframes(CANDIDATE_DIR)\n",
    "minhashes = []\n",
    "index_metadata = []\n",
    "\n",
    "for table_name, col_name, values, size,size_with_na in column_entries:\n",
    "    mh = create_minhash(values, NUM_PERM)\n",
    "    minhashes.append(mh)\n",
    "    index_metadata.append({\n",
    "        'table': table_name,\n",
    "        'column': col_name,\n",
    "        'full_name': f\"{table_name}.{col_name}\",\n",
    "        'size': size,\n",
    "        'keys': [col_name] , \n",
    "        'size_with_na':size_with_na\n",
    "    })\n",
    "\n",
    "lsh = MinHashLSHEnsemble(threshold=0.1, num_perm=NUM_PERM)\n",
    "keys = [m['full_name'] for m in index_metadata]\n",
    "sizes = [m['size'] for m in index_metadata]\n",
    "combined = list(zip(keys, minhashes, sizes))\n",
    "lsh.index(combined) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing new attribute: `Director_Awards`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Awards | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      " → movie_actor_bridge.Actor_ID | Jaccard: 0.1094 | Join: inner| Confidence: 37.5\n",
      "\n",
      "✅ Best match for `Director_Awards`: directors.Director_Awards → obtained trough left join on original_table.Series_Title=directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Birth_Year`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Birth_Year | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Director_Birth_Year`: directors.Director_Birth_Year → obtained trough left join on original_table.Series_Title=directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Nationality`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Nationality | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      " → country_data.Country | Jaccard: 0.6953 | Join: inner| Confidence: 62.5\n",
      "\n",
      "✅ Best match for `Director_Nationality`: directors.Director_Nationality → obtained trough left join on original_table.Series_Title=directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Gender`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Gender | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Director_Gender`: directors.Director_Gender → obtained trough left join on original_table.Series_Title=directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Name`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Name | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Director_Name`: directors.Director_Name → obtained trough left join on original_table.Series_Title=directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Profitability_Ratio`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Profitability_Ratio | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Profitability_Ratio`: movie_financials.Profitability_Ratio → obtained trough left join on original_table.Series_Title=movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Release_Season`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Release_Season | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Release_Season`: movie_financials.Release_Season → obtained trough left join on original_table.Series_Title=movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Production_Budget`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Production_Budget | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Production_Budget`: movie_financials.Production_Budget → obtained trough left join on original_table.Series_Title=movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Primary_Production_Company`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Primary_Production_Company | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Primary_Production_Company`: movie_financials.Primary_Production_Company → obtained trough left join on original_table.Series_Title=movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Box_Office_Gross`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Box_Office_Gross | Jaccard: 1.0000 | Join: left| Confidence: 80\n",
      "\n",
      "✅ Best match for `Box_Office_Gross`: movie_financials.Box_Office_Gross → obtained trough left join on original_table.Series_Title=movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--- For each new attribute: search + recommend join ---\n",
    "for new_col in added_cols:\n",
    "    print(f\"🔍 Analyzing new attribute: `{new_col}`\")\n",
    "\n",
    "    new_values = new_df[new_col].astype(str).dropna().unique().tolist()\n",
    "    if not new_values:\n",
    "        print(\"⚠️ No values found for this attribute. Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    new_attr_minhash = create_minhash(new_values, NUM_PERM)\n",
    "    candidates=[]\n",
    "    for key in lsh.query(new_attr_minhash, len(new_values)):\n",
    "        candidates.append(key)\n",
    "    \n",
    "    #rank candidate tables based on jaccard similarity\n",
    "    ranked = []\n",
    "    for meta, mh in zip(index_metadata, minhashes):\n",
    "        if meta['full_name'] in candidates:\n",
    "            sim = new_attr_minhash.jaccard(mh)\n",
    "            #join_type = recommend_join_type(meta['keys'], MAIN_DATASET_KEYS, sim)\n",
    "            join_type =estimate_join_type_with_confidence(len(base_df), len(new_df), meta['size_with_na'])\n",
    "            join_attribute=find_joinable_attributes(base_df,get_table(meta['table']))\n",
    "            if join_attribute:\n",
    "                ranked.append((meta['table'], meta['column'], sim, join_type,join_attribute))\n",
    "\n",
    "    ranked.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n",
    "    if ranked:\n",
    "        print(\"\\nTop Candidate Matches:\")\n",
    "        for table, column, sim, join_type,join_attribute in ranked[:5]:\n",
    "            #print(f\" → {table}.{column} | Jaccard: {sim:.4f} | Join: {join_type}\")\n",
    "            print(f\" → {table}.{column} | Jaccard: {sim:.4f} | Join: {join_type[0][0]}| Confidence: {join_type[0][1]}\")\n",
    "\n",
    "        best = ranked[0]\n",
    "        print(f\"\\n✅ Best match for `{new_col}`: {best[0]}.{best[1]} → obtained trough {best[3][0][0]} join on original_table.{best[4][0][0]} = {best[0]}.{best[4][0][1]} (Jaccard sim: {best[2]:.4f})\\n\")\n",
    "    else:\n",
    "        print(\"❌ No good matches found for this attribute.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
