{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22020b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analyzing: D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/Versions/imdb_with_financials_and_directors.csv\n",
      "==================================================\n",
      "\n",
      "🆕 New attributes detected: Director_Name, Release_Season, Director_Awards, Profitability_Ratio, Production_Budget, Director_Nationality, Director_Birth_Year, Primary_Production_Company, Box_Office_Gross, Director_Gender\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Name`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Name | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Director_Name`: directors.Director_Name → obtained through left join on original_table.Series_Title = directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Release_Season`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Release_Season | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Release_Season`: movie_financials.Release_Season → obtained through left join on original_table.Series_Title = movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Awards`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Awards | Jaccard: 1.0000 | Join: left\n",
      " → movie_actor_bridge.Actor_ID | Jaccard: 0.1094 | Join: inner\n",
      "\n",
      "✅ Best match for `Director_Awards`: directors.Director_Awards → obtained through left join on original_table.Series_Title = directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Profitability_Ratio`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Profitability_Ratio | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Profitability_Ratio`: movie_financials.Profitability_Ratio → obtained through left join on original_table.Series_Title = movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Production_Budget`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Production_Budget | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Production_Budget`: movie_financials.Production_Budget → obtained through left join on original_table.Series_Title = movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Nationality`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Nationality | Jaccard: 1.0000 | Join: left\n",
      " → country_data.Country | Jaccard: 0.6953 | Join: inner\n",
      "\n",
      "✅ Best match for `Director_Nationality`: directors.Director_Nationality → obtained through left join on original_table.Series_Title = directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Birth_Year`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Birth_Year | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Director_Birth_Year`: directors.Director_Birth_Year → obtained through left join on original_table.Series_Title = directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Primary_Production_Company`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Primary_Production_Company | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Primary_Production_Company`: movie_financials.Primary_Production_Company → obtained through left join on original_table.Series_Title = movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Box_Office_Gross`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → movie_financials.Box_Office_Gross | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Box_Office_Gross`: movie_financials.Box_Office_Gross → obtained through left join on original_table.Series_Title = movie_financials.Series_Title (Jaccard sim: 1.0000)\n",
      "\n",
      "🔍 Analyzing new attribute: `Director_Gender`\n",
      "\n",
      "Top Candidate Matches:\n",
      " → directors.Director_Gender | Jaccard: 1.0000 | Join: left\n",
      "\n",
      "✅ Best match for `Director_Gender`: directors.Director_Gender → obtained through left join on original_table.Series_Title = directors.Series_Title (Jaccard sim: 1.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSHEnsemble\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "\n",
    "class ExternalAttributeAnalyzer:\n",
    "    def __init__(self, num_perm: int = 128):\n",
    "        self.NUM_PERM = num_perm\n",
    "    \n",
    "    def create_minhash(self, values: List[str], num_perm: int = None) -> MinHash:\n",
    "        \"\"\"Create a MinHash object from a list of values.\"\"\"\n",
    "        if num_perm is None:\n",
    "            num_perm = self.NUM_PERM\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for v in values:\n",
    "            if pd.notna(v):\n",
    "                m.update(str(v).strip().lower().encode('utf8'))\n",
    "        return m\n",
    "    \n",
    "    def extract_column_dataframes(self, directory: str) -> List[Tuple[str, str, List[str], int, int]]:\n",
    "        \"\"\"Extract column information from all CSV files in a directory.\"\"\"\n",
    "        results = []\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                table_name = filename[:-4]\n",
    "                df = pd.read_csv(os.path.join(directory, filename))\n",
    "                for col in df.columns:\n",
    "                    values = df[col].astype(str).dropna().unique().tolist()\n",
    "                    results.append((table_name, col, values, len(values), len(df)))\n",
    "        return results\n",
    "    \n",
    "    def get_table(self, candidate_dir: str, name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load a table from the candidate directory.\"\"\"\n",
    "        name = name + \".csv\"\n",
    "        return pd.read_csv(os.path.join(candidate_dir, name))\n",
    "    \n",
    "    def get_most_probable_join_type(self,orig_rows, ext_rows, joined_rows):\n",
    "        \"\"\"\n",
    "        Returns the most probable join type with its relative probability (0-1)\n",
    "        based solely on row counts, using statistical likelihoods.\n",
    "    \n",
    "        Args:\n",
    "            orig_rows: Rows in left table\n",
    "            ext_rows: Rows in right table\n",
    "            joined_rows: Rows in joined result\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (most_probable_join_type, probability)\n",
    "        \"\"\"\n",
    "        # Calculate boundary conditions\n",
    "        min_rows = min(orig_rows, ext_rows)\n",
    "        max_rows = max(orig_rows, ext_rows)\n",
    "        sum_rows = orig_rows + ext_rows\n",
    "        cross_rows = orig_rows * ext_rows\n",
    "    \n",
    "        # Initialize likelihoods (not normalized)\n",
    "        likelihoods = {\n",
    "            'inner': 0.0,\n",
    "            'left': 0.0,\n",
    "            'right': 0.0,\n",
    "            'full': 0.0,\n",
    "            'cross': 1e-9  # negligible base value\n",
    "        }\n",
    "        \n",
    "        # 1. Inner join likelihood (normal distribution around expected matches)\n",
    "        if min_rows > 0:\n",
    "            expected_inner = min_rows * 0.5  # Prior assumption: 50% of keys match\n",
    "            std_inner = min_rows * 0.3       # Reasonable standard deviation\n",
    "            likelihoods['inner'] = np.exp(-((joined_rows - expected_inner)**2)/(2*std_inner**2))\n",
    "        \n",
    "        # 2. Left join likelihood (exponential decay from perfect match)\n",
    "        likelihoods['left'] = np.exp(-0.5 * abs(joined_rows - orig_rows)/orig_rows) if orig_rows > 0 else 0\n",
    "        \n",
    "        # 3. Right join likelihood (exponential decay from perfect match)\n",
    "        likelihoods['right'] = np.exp(-0.5 * abs(joined_rows - ext_rows)/ext_rows) if ext_rows > 0 else 0\n",
    "        \n",
    "        # 4. Full outer join likelihood (triangular distribution)\n",
    "        if max_rows < sum_rows:\n",
    "            if joined_rows <= max_rows:\n",
    "                likelihoods['full'] = joined_rows / sum_rows\n",
    "            else:\n",
    "                likelihoods['full'] = (sum_rows - joined_rows) / sum_rows\n",
    "        \n",
    "        # 5. Cross join (only possible if exact product)\n",
    "        if joined_rows == cross_rows:\n",
    "            likelihoods['cross'] = 1.0\n",
    "        \n",
    "        # Normalize to probabilities\n",
    "        total_likelihood = sum(likelihoods.values())\n",
    "        probabilities = {k: v/total_likelihood for k, v in likelihoods.items()}\n",
    "        \n",
    "        # Return the most probable\n",
    "        most_probable = max(probabilities.items(), key=lambda x: x[1])\n",
    "        return most_probable\n",
    "    \n",
    "\n",
    "    \n",
    "    def __overlap(self,column1:pd.DataFrame,column2:pd.DataFrame,sample_size: int = 1000) -> int:\n",
    "        sample1 = self._get_sample(column1.dropna(), sample_size)\n",
    "        sample2 = self._get_sample(column2.dropna(), sample_size)\n",
    "                    \n",
    "        # Skip if either sample is empty\n",
    "        if len(sample1) == 0 or len(sample2) == 0:\n",
    "            return 0\n",
    "                    \n",
    "        # Check for overlapping values\n",
    "        common_values = set(sample1) & set(sample2)\n",
    "        if len(common_values) > 0:\n",
    "            overlap_ratio = len(common_values) / min(len(sample1.unique()), len(sample2.unique()))\n",
    "        else:\n",
    "            return 0\n",
    "        return overlap_ratio\n",
    "\n",
    "    \n",
    "    def find_joinable_attributes(self, df1: pd.DataFrame, df2: pd.DataFrame, \n",
    "                               sample_size: int = 1000, min_overlap: float = 0.1) -> List[Tuple[str, str, float]]:\n",
    "        \"\"\"Find potential joinable attributes between two DataFrames.\"\"\"\n",
    "        potential_matches = []\n",
    "        \n",
    "        # Step 1: Find columns with matching names and compatible dtypes\n",
    "        common_cols = set(df1.columns) & set(df2.columns)\n",
    "        for col in common_cols:\n",
    "            if pd.api.types.is_dtype_equal(df1[col].dtype, df2[col].dtype):\n",
    "                overlap_ratio=self.__overlap(df1[col],df2[col],sample_size)\n",
    "                if overlap_ratio >= min_overlap:\n",
    "                    potential_matches.append((col, col, overlap_ratio))\n",
    "                #potential_matches.append((col, col, 0))\n",
    "        \n",
    "        # Step 2: Find columns with compatible dtypes (even if names don't match)\n",
    "        for col1 in df1.columns:\n",
    "            for col2 in df2.columns:\n",
    "                # Skip if already found or same column pair\n",
    "                if (col1, col2) in [(x[0], x[1]) for x in potential_matches] or col1 == col2:\n",
    "                    continue\n",
    "                \n",
    "                # Check dtype compatibility\n",
    "                if pd.api.types.is_dtype_equal(df1[col1].dtype, df2[col2].dtype):\n",
    "                    # Get samples (handle empty DataFrames)\n",
    "                    overlap_ratio=self.__overlap(df1[col1],df2[col2],sample_size)\n",
    "                    if overlap_ratio >= min_overlap:\n",
    "                        potential_matches.append((col1, col2, overlap_ratio))\n",
    "            potential_matches.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        return potential_matches\n",
    "    \n",
    "    def _get_sample(self, series: pd.Series, sample_size: int) -> pd.Series:\n",
    "        \"\"\"Helper function to get sample from a series.\"\"\"\n",
    "        if sample_size <= 0 or len(series) <= sample_size:\n",
    "            return series\n",
    "        return series.sample(sample_size)\n",
    "    \n",
    "    def analyze_new_attributes(self, base_file: str, new_file: str, candidate_dir: str) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Analyze new attributes between base and new files, searching for matches in candidate tables.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with analysis results for each new attribute\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        base_df = pd.read_csv(base_file)\n",
    "        new_df = pd.read_csv(new_file)\n",
    "        \n",
    "        # Detect newly added attributes\n",
    "        base_cols = set(base_df.columns)\n",
    "        new_cols = set(new_df.columns)\n",
    "        added_cols = new_cols - base_cols\n",
    "        \n",
    "        if not added_cols:\n",
    "            return {'status': 'no_new_attributes', 'results': {}}\n",
    "        \n",
    "        # Index external candidate columns with MinHash + LSH Ensemble\n",
    "        column_entries = self.extract_column_dataframes(candidate_dir)\n",
    "        minhashes = []\n",
    "        index_metadata = []\n",
    "        \n",
    "        for table_name, col_name, values, size, size_with_na in column_entries:\n",
    "            mh = self.create_minhash(values)\n",
    "            minhashes.append(mh)\n",
    "            index_metadata.append({\n",
    "                'table': table_name,\n",
    "                'column': col_name,\n",
    "                'full_name': f\"{table_name}.{col_name}\",\n",
    "                'size': size,\n",
    "                'keys': [col_name],\n",
    "                'size_with_na': size_with_na\n",
    "            })\n",
    "        \n",
    "        lsh = MinHashLSHEnsemble(threshold=0.1, num_perm=self.NUM_PERM)\n",
    "        keys = [m['full_name'] for m in index_metadata]\n",
    "        sizes = [m['size'] for m in index_metadata]\n",
    "        combined = list(zip(keys, minhashes, sizes))\n",
    "        lsh.index(combined)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # For each new attribute: search + recommend join\n",
    "        for new_col in added_cols:\n",
    "            results[new_col] = {'matches': []}\n",
    "\n",
    "            new_values = new_df[new_col].astype(str).dropna().unique().tolist()\n",
    "            if not new_values:\n",
    "                results[new_col]['warning'] = \"No values found\"\n",
    "                continue\n",
    "\n",
    "            new_attr_minhash = self.create_minhash(new_values)\n",
    "            candidates = list(lsh.query(new_attr_minhash, len(new_values)))\n",
    "            \n",
    "            # Rank candidate tables based on jaccard similarity\n",
    "            ranked = []\n",
    "            for meta, mh in zip(index_metadata, minhashes):\n",
    "                if meta['full_name'] in candidates:\n",
    "                    sim = new_attr_minhash.jaccard(mh)\n",
    "                    join_type = self.get_most_probable_join_type(len(base_df), len(new_df), meta['size_with_na'])\n",
    "                    join_attribute = self.find_joinable_attributes(base_df, self.get_table(candidate_dir, meta['table']))\n",
    "                    \n",
    "                    if join_attribute:\n",
    "                        ranked.append({\n",
    "                            'table': meta['table'],\n",
    "                            'column': meta['column'],\n",
    "                            'jaccard_sim': sim,\n",
    "                            'join_type': join_type,\n",
    "                            'join_attributes': join_attribute,\n",
    "                        })\n",
    "\n",
    "            ranked.sort(key=lambda x: x['jaccard_sim'], reverse=True)\n",
    "            results[new_col]['matches'] = ranked\n",
    "            \n",
    "            if not ranked:\n",
    "                results[new_col]['warning'] = \"No good matches found\"\n",
    "        \n",
    "        return {'status': 'success', 'results': results, 'added_columns': list(added_cols)}\n",
    "\n",
    "\n",
    "class ResultPrinter:\n",
    "    @staticmethod\n",
    "    def print_analysis_results(results: Dict):\n",
    "        \"\"\"Print the analysis results in a human-readable format.\"\"\"\n",
    "        if results['status'] == 'no_new_attributes':\n",
    "            print(\"✅ No new attributes found.\")\n",
    "            return\n",
    "        \n",
    "        added_cols = results.get('added_columns', [])\n",
    "        if not added_cols:\n",
    "            print(\"No new attributes to analyze.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"🆕 New attributes detected: {', '.join(added_cols)}\\n\")\n",
    "        \n",
    "        for new_col, data in results['results'].items():\n",
    "            print(f\"🔍 Analyzing new attribute: `{new_col}`\")\n",
    "            \n",
    "            if 'warning' in data:\n",
    "                if data['warning'] == \"No values found\":\n",
    "                    print(\"⚠️ No values found for this attribute. Skipping.\\n\")\n",
    "                elif data['warning'] == \"No good matches found\":\n",
    "                    print(\"❌ No good matches found for this attribute.\\n\")\n",
    "                continue\n",
    "            \n",
    "            if not data['matches']:\n",
    "                print(\"❌ No good matches found for this attribute.\\n\")\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nTop Candidate Matches:\")\n",
    "            for match in data['matches'][:5]:\n",
    "                print(f\" → {match['table']}.{match['column']} | Jaccard: {match['jaccard_sim']:.4f} | Join: {match['join_type'][0]}\")\n",
    "\n",
    "            best = data['matches'][0]\n",
    "            print(f\"\\n✅ Best match for `{new_col}`: {best['table']}.{best['column']} → obtained through {best['join_type'][0]} join on original_table.{best['join_attributes'][0][0]} = {best['table']}.{best['join_attributes'][0][1]} (Jaccard sim: {best['jaccard_sim']:.4f})\\n\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer and printer\n",
    "    analyzer = ExternalAttributeAnalyzer(num_perm=128)\n",
    "    printer = ResultPrinter()\n",
    "    \n",
    "    # Define your configurations\n",
    "    configurations = [\n",
    "        {\n",
    "            \"base_file\": \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/IMDB_Base.csv\",\n",
    "            \"new_file\": \"D:/uni/AdvancedTopics/project2/sinteticDB/IMDB/Versions/imdb_with_financials_and_directors.csv\",\n",
    "            \"candidate_dir\": \"sinteticDB/IMDB/externalTables\"\n",
    "        },\n",
    "        # Add more configurations as needed\n",
    "    ]\n",
    "    \n",
    "    # Process each configuration\n",
    "    for config in configurations:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analyzing: {config['new_file']}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "        \n",
    "        # Perform analysis (no printing here)\n",
    "        results = analyzer.analyze_new_attributes(\n",
    "            base_file=config['base_file'],\n",
    "            new_file=config['new_file'],\n",
    "            candidate_dir=config['candidate_dir']\n",
    "        )\n",
    "        \n",
    "        # Now print the results when we want to\n",
    "        printer.print_analysis_results(results)\n",
    "        \n",
    "        # You can also access the results programmatically\n",
    "        # for new_col, data in results['results'].items():\n",
    "        #     # Process results as needed\n",
    "        #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9fb6c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Director_Name': {'matches': [{'table': 'directors',\n",
       "    'column': 'Director_Name',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Release_Season': {'matches': [{'table': 'movie_financials',\n",
       "    'column': 'Release_Season',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Director_Awards': {'matches': [{'table': 'directors',\n",
       "    'column': 'Director_Awards',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]},\n",
       "   {'table': 'movie_actor_bridge',\n",
       "    'column': 'Actor_ID',\n",
       "    'jaccard_sim': 0.109375,\n",
       "    'join_type': ('inner', 0.3175351663943463),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Profitability_Ratio': {'matches': [{'table': 'movie_financials',\n",
       "    'column': 'Profitability_Ratio',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Production_Budget': {'matches': [{'table': 'movie_financials',\n",
       "    'column': 'Production_Budget',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Director_Nationality': {'matches': [{'table': 'directors',\n",
       "    'column': 'Director_Nationality',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]},\n",
       "   {'table': 'country_data',\n",
       "    'column': 'Country',\n",
       "    'jaccard_sim': 0.6953125,\n",
       "    'join_type': ('inner', 0.3570883842864344),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0),\n",
       "     ('Released_Year', 'Censorship_Rating', 0.16666666666666666)]}]},\n",
       " 'Director_Birth_Year': {'matches': [{'table': 'directors',\n",
       "    'column': 'Director_Birth_Year',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Primary_Production_Company': {'matches': [{'table': 'movie_financials',\n",
       "    'column': 'Primary_Production_Company',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Box_Office_Gross': {'matches': [{'table': 'movie_financials',\n",
       "    'column': 'Box_Office_Gross',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]},\n",
       " 'Director_Gender': {'matches': [{'table': 'directors',\n",
       "    'column': 'Director_Gender',\n",
       "    'jaccard_sim': 1.0,\n",
       "    'join_type': ('left', 0.36372204203003966),\n",
       "    'join_attributes': [('Series_Title', 'Series_Title', 1.0)]}]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae15bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BIRD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
